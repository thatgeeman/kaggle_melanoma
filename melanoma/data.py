# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_train.ipynb.

# %% auto 0
__all__ = ['NUM_WORKERS', 'ClassificationDataset', 'ClassificationDataModule', 'ClassificationModel']

# %% ../nbs/02_train.ipynb 3
from typing import Optional, Any, Union, Iterable, List
from fastcore.utils import store_attr

import torch

# %% ../nbs/02_train.ipynb 4
from PIL import Image
from fastcore.utils import Path

import numpy as np
import albumentations as A
import matplotlib.pyplot as plt

from torch.utils.data import DataLoader

# %% ../nbs/02_train.ipynb 7
class ClassificationDataset:
    def __init__(
        self,
        paths: List,
        labels: Optional[List] = None,
        augs: Optional[Any] = None,
    ):
        store_attr()

    def __getitem__(self, idx) -> Union[torch.Tensor, torch.Tensor]:
        path = self.paths[idx]
        # load image
        img = np.asarray(Image.open(path), dtype=np.float32)
        # apply aug
        if self.augs:
            img = self.augs(image=img)
            img = img["image"]
        # make tensor
        img = torch.FloatTensor(img).permute((2, 0, 1))
        # get labels if available
        if self.labels is not None:
            label = self.labels[idx]
            label = torch.LongTensor([label])
            return (img, label)
        return img

    def __len__(self) -> int:
        return len(self.paths)

    def show(self, idx):
        path = self.paths[idx]
        # load image
        img = np.asarray(Image.open(path), dtype=np.uint8)
        # apply aug
        if self.augs:
            img = self.augs(image=img)
            img = img["image"]
        return plt.imshow(img)


# %% ../nbs/02_train.ipynb 18
from typing import Dict 
import pytorch_lightning as pl

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score

# %% ../nbs/02_train.ipynb 19
from . import utils
import os

# %% ../nbs/02_train.ipynb 20
NUM_WORKERS = os.cpu_count()

# %% ../nbs/02_train.ipynb 22
class ClassificationDataModule(pl.LightningDataModule):
    def __init__(self, workdir, n_splits: int = 5, limit_samples: Optional = None):
        super().__init__()
        self.workdir = Path(workdir)
        self.n_splits = n_splits
        self.limit_samples = limit_samples

    def prepare_data(self):
        # download, tokenize, etc
        pass

    def setup(self, stage=None):
        # stage can be either 'fit', 'validate', 'test', or 'predict'
        # split, transform etc
        df = utils.create_folds(self.workdir, self.n_splits)
        # if testing, useful to avoid looping over entire dataset
        df = (
            df.sample(self.limit_samples, replace=False).reset_index(drop=True)
            if self.limit_samples is not None
            else df
        )
        # not K-fold for now
        tr_df = df.query("fold!=0").drop(columns=["fold"]).reset_index(drop=True)
        tr_paths = tr_df.image_name.apply(
            lambda x: self.workdir / f"train/{x}.jpg"
        ).to_list()
        tr_labels = tr_df.target.to_list()

        val_df = df.query("fold==0").drop(columns=["fold"]).reset_index(drop=True)
        val_paths = val_df.image_name.apply(
            lambda x: self.workdir / f"train/{x}.jpg"
        ).to_list()
        val_labels = val_df.target.to_list()

        tr_augs = A.Compose(
            [
                A.Normalize(always_apply=True),
                A.RandomBrightnessContrast(),
                A.Rotate(limit=45),
            ]
        )

        val_augs = A.Compose(
            [
                A.Normalize(always_apply=True),
            ]
        )

        self.tr_dset = ClassificationDataset(
            paths=tr_paths, labels=tr_labels, augs=tr_augs
        )

        self.val_dset = ClassificationDataset(
            paths=val_paths, labels=val_labels, augs=val_augs
        )

    def train_dataloader(self):
        return DataLoader(
            self.tr_dset,
            batch_size=32,
            num_workers=NUM_WORKERS,
            shuffle=True,
            drop_last=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dset,
            batch_size=32,
            num_workers=NUM_WORKERS,
            shuffle=True,
            drop_last=True,
        )

    def test_dataloader(self):
        return None


# %% ../nbs/02_train.ipynb 26
from torchvision import models
from torch import nn
import torch.nn.functional as F

# %% ../nbs/02_train.ipynb 32
class ClassificationModel(pl.LightningModule):
    def __init__(self, model, loss_weights=None):
        super().__init__()
        self.model = model
        self.loss_weights = torch.Tensor(loss_weights)

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        acts = self(x)  # batch_size, n_classes
        loss = nn.CrossEntropyLoss(weight=self.loss_weights)(acts, y.squeeze())
        probas = F.softmax(acts, -1)
        preds = probas.argmax(-1)
        return {
            "loss": loss,
            "train_preds": preds.squeeze().tolist(),
            "train_probas": probas[:, 1].detach().tolist(),
            "train_labels": y.squeeze().tolist(),
        }

    def validation_step(self, batch, batch_idx):
        x, y = batch
        acts = self(x)  # batch_size, n_classes
        loss = nn.CrossEntropyLoss(weight=self.loss_weights)(acts, y.squeeze())
        probas = F.softmax(acts, -1)
        preds = probas.argmax(-1)
        return {
            "valid_loss": loss,
            "valid_preds": preds.squeeze().tolist(),
            "valid_probas": probas[:, 1].detach().tolist(),
            "valid_labels": y.squeeze().tolist(),
        }

    def training_epoch_end(self, outputs):
        # outputs: a dict for each step
        if not self.trainer.sanity_checking:
            loss = np.array([x["loss"].item() for x in outputs]).mean()
            probas = np.array([x["train_probas"] for x in outputs]).reshape(-1)
            preds = np.array([x["train_preds"] for x in outputs]).reshape(-1)
            target = np.array([x["train_labels"] for x in outputs]).reshape(-1)
            self.log("train_loss", loss, on_step=False, on_epoch=True)
            self.log(
                "train_auc", roc_auc_score(target, probas), on_step=False, on_epoch=True
            )

    def validation_epoch_end(self, outputs):
        # outputs: a dict for each step
        if not self.trainer.sanity_checking:
            loss = np.array([x["valid_loss"].item() for x in outputs]).mean()
            probas = np.array([x["valid_probas"] for x in outputs]).reshape(-1)
            preds = np.array([x["valid_preds"] for x in outputs]).reshape(-1)
            target = np.array([x["valid_labels"] for x in outputs]).reshape(-1)
            self.log("valid_loss", loss, on_step=False, on_epoch=True)
            self.log(
                "valid_auc", roc_auc_score(target, probas), on_step=False, on_epoch=True
            )

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

